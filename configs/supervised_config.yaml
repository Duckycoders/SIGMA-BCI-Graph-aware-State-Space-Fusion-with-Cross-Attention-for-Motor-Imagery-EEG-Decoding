# 有监督联合训练配置
model:
  dataset_type: 'bnci2a'
  n_channels: 22
  n_samples: 751  # 使用实际数据长度，避免长度不匹配
  sfreq: 250.0
  
  # FilterBank配置
  filter_bands: [[4, 8], [8, 14], [14, 30], [30, 45]]
  use_adaptive_filterbank: false
  
  # 图卷积配置 - 启用并优化
  use_graph_conv: true  # 启用图卷积
  graph_hidden_dims: [32, 64]  # 适中维度
  graph_output_dim: 64         # 输出维度
  
  # S4分支配置 - 简化模型维度
  use_s4_branch: true
  s4_d_model: 64   # 优化维度
  s4_d_state: 32   # 优化状态维度
  s4_n_layers: 2
  
  # Mamba分支配置 - 启用并优化
  use_mamba_branch: true  # 启用Mamba
  mamba_d_model: 64  # 优化维度
  mamba_d_state: 16
  mamba_n_layers: 2
  
  # 融合配置 - 减少attention头数
  fusion_method: 'cross_attention'
  fusion_n_heads: 4  # 减少从8到4
  
  # MoE配置 - 简化以减少过拟合
  use_moe: true
  moe_n_experts: 2  # 减少专家数量从4到2
  moe_top_k: 1      # 减少top_k从2到1
  
  # 多任务配置
  task_configs:
    bnci2a:
      d_output: 4
      top_k: 2
    bnci2b:
      d_output: 2
      top_k: 2
    eegmmi:
      d_output: 4
      top_k: 2
  
  # 通用配置 - 增加正则化
  dropout: 0.2  # 增加dropout从0.1到0.2

training:
  # 优化器配置 - 优化稳定性
  learning_rate: 5e-4      # 降低学习率从1e-3到5e-4
  weight_decay: 1e-3       # 增加weight_decay从1e-4到1e-3
  
  # 训练配置 - RTX 3060 Laptop优化
  batch_size: 8            # 笔记本GPU较小batch_size
  max_epochs: 30           # 增加epoch数但使用早停
  early_stopping_patience: 8  # 增加早停patience
  
  # 正则化配置
  gradient_clip_val: 1.0   # 添加梯度裁剪
  early_stopping_min_delta: 0.001  # 早停最小改进
  
  # 调度器配置
  scheduler_type: 'cosine'  # 'cosine', 'step', 'plateau'
  
  # 损失配置
  label_smoothing: 0.1
  
  # 任务权重
  task_weights:
    bnci2a: 1.0
    bnci2b: 1.5  # 更小的数据集给更高权重
    eegmmi: 1.0
    default: 1.0  # 默认任务权重

augmentation:
  enabled: true
  augment_prob: 0.3  # 简化的增强概率

